import os
import json
import logging
from typing import List, Dict, Any
from fastapi import FastAPI, Body, HTTPException
from pydantic import BaseModel
import chromadb
from sentence_transformers import SentenceTransformer
from pathlib import Path

# Configuraci√≥n
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO"))
logger = logging.getLogger(__name__)

# Modelos Pydantic
class QueryRequest(BaseModel):
    query: str
    max_results: int = 3

class RAGService:
    """Servicio RAG que gestiona embeddings y b√∫squeda sem√°ntica."""
    def __init__(self):
        logger.info("Inicializando RAG Service...")
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.client = chromadb.EphemeralClient()
        self.collection = self.client.get_or_create_collection(
            name="wine_knowledge_v2",
            metadata={"hnsw:space": "cosine"}
        )
        self._load_initial_data()

    def _process_enology_simple(self, text_path: Path):
        """Procesa el texto de maestr√≠a enol√≥gica con estrategia simple por p√°rrafos."""
        logger.info(f"üìñ Procesando maestr√≠a enol√≥gica desde {text_path}...")
        
        try:
            with open(text_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Dividir por p√°rrafos dobles (estrategia m√°s simple)
            paragraphs = content.split('\n\n')
            
            ids_to_add, docs_to_add, metas_to_add = [], [], []
            existing_ids = set(self.collection.get(include=[])['ids'])
            
            chunk_count = 0
            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                
                # Solo procesar p√°rrafos con contenido sustancial
                if len(paragraph) > 100:  # M√≠nimo 100 caracteres
                    chunk_id = f"enologia_paragraph_{chunk_count}"
                    
                    if chunk_id not in existing_ids:
                        # Si el p√°rrafo es muy largo (>800 chars), dividirlo por oraciones
                        if len(paragraph) > 800:
                            sentences = paragraph.replace('. ', '.\n').split('\n')
                            current_text = ""
                            
                            for sentence in sentences:
                                sentence = sentence.strip()
                                if len(current_text + sentence) > 800 and current_text:
                                    # Guardar chunk actual
                                    sub_chunk_id = f"enologia_paragraph_{chunk_count}_part"
                                    metadata = {
                                        'type': 'knowledge',
                                        'category': 'enologia',
                                        'chunk_type': 'paragraph_split',
                                        'estimated_chars': len(current_text)
                                    }
                                    
                                    ids_to_add.append(sub_chunk_id)
                                    docs_to_add.append(current_text.strip())
                                    metas_to_add.append(metadata)
                                    
                                    chunk_count += 1
                                    current_text = sentence + " "
                                else:
                                    current_text += sentence + " "
                            
                            # A√±adir √∫ltimo fragmento si existe
                            if current_text.strip():
                                sub_chunk_id = f"enologia_paragraph_{chunk_count}_final"
                                metadata = {
                                    'type': 'knowledge',
                                    'category': 'enologia', 
                                    'chunk_type': 'paragraph_final',
                                    'estimated_chars': len(current_text)
                                }
                                
                                ids_to_add.append(sub_chunk_id)
                                docs_to_add.append(current_text.strip())
                                metas_to_add.append(metadata)
                                chunk_count += 1
                        else:
                            # P√°rrafo normal, a√±adir directamente
                            metadata = {
                                'type': 'knowledge',
                                'category': 'enologia',
                                'chunk_type': 'paragraph',
                                'estimated_chars': len(paragraph)
                            }
                            
                            ids_to_add.append(chunk_id)
                            docs_to_add.append(paragraph)
                            metas_to_add.append(metadata)
                            chunk_count += 1
            
            # A√±adir todos los chunks a la colecci√≥n
            if ids_to_add:
                self.collection.add(ids=ids_to_add, documents=docs_to_add, metadatas=metas_to_add)
                logger.info(f"‚úÖ A√±adidos {len(ids_to_add)} chunks de conocimiento enol√≥gico")
            else:
                logger.info("‚úÖ El conocimiento enol√≥gico ya estaba cargado")
                
        except Exception as e:
            logger.error(f"‚ùå Error procesando maestr√≠a enol√≥gica: {e}")
            logger.info("üîÑ Continuando sin conocimiento enol√≥gico")

    def _semantic_chunk(self, section: Dict, max_tokens: int = 1000, overlap: int = 75) -> List[Dict]:
        """Divide una secci√≥n en chunks sem√°nticos manteniendo contexto."""
        content = section['content']
        
        # Dividir por p√°rrafos para mantener coherencia sem√°ntica
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        # Estimar tokens (aproximadamente 4 caracteres por token)
        def estimate_tokens(text):
            return len(text) // 4
        
        for paragraph in paragraphs:
            para_tokens = estimate_tokens(paragraph)
            
            # Si el p√°rrafo solo excede el l√≠mite, dividirlo por oraciones
            if para_tokens > max_tokens:
                sentences = re.split(r'[.!?]+\s+', paragraph)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                        
                    sent_tokens = estimate_tokens(sentence)
                    
                    if current_tokens + sent_tokens > max_tokens and current_chunk:
                        # Guardar chunk actual
                        chunks.append({
                            'content': current_chunk.strip(),
                            'tokens': current_tokens
                        })
                        current_chunk = sentence + ". "
                        current_tokens = sent_tokens
                    else:
                        current_chunk += sentence + ". "
                        current_tokens += sent_tokens
            
            # Si agregar este p√°rrafo excede el l√≠mite
            elif current_tokens + para_tokens > max_tokens and current_chunk:
                # Guardar chunk actual
                chunks.append({
                    'content': current_chunk.strip(),
                    'tokens': current_tokens
                })
                current_chunk = paragraph + "\n\n"
                current_tokens = para_tokens
            else:
                current_chunk += paragraph + "\n\n"
                current_tokens += para_tokens
        
        # A√±adir √∫ltimo chunk si existe
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'tokens': current_tokens
            })
        
        # A√±adir metadatos de secci√≥n a cada chunk
        for chunk in chunks:
            chunk.update({
                'section_id': section['id'],
                'main_title': section['main_title'],
                'sub_title': section['sub_title'],
                'full_title': section['full_title']
            })
        
        return chunks

    def _extract_topic_keywords(self, content: str) -> List[str]:
        """Extrae palabras clave principales del contenido."""
        # Palabras clave comunes del dominio enol√≥gico
        wine_keywords = [
            'sumiller', 'sommelier', 'maridaje', 'cata', 'vino', 'bodega', 'uva', 'variedad',
            'terru√±o', 'terroir', 'acidez', 'taninos', 'crianza', 'fermentaci√≥n', 'barrica',
            'degustaci√≥n', 'aroma', 'sabor', 'textura', 'servicio', 'temperatura', 'copa',
            'decantaci√≥n', 'a√±ada', 'cosecha', 'vendimia', 'en√≥logo', 'vinificaci√≥n',
            'equilibrio', 'intensidad', 'complementariedad', 'contraste', 'grasa', 'prote√≠na'
        ]
        
        content_lower = content.lower()
        found_keywords = []
        
        for keyword in wine_keywords:
            if keyword in content_lower:
                found_keywords.append(keyword)
        
        return found_keywords[:5]  # M√°ximo 5 keywords principales

    def _process_enology_text(self, text_path: Path):
        """Procesa el texto de maestr√≠a enol√≥gica con chunking sem√°ntico."""
        logger.info(f"Procesando texto de maestr√≠a enol√≥gica desde {text_path}...")
        
        with open(text_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extraer secciones del documento
        sections = self._extract_sections_from_enology_text(content)
        logger.info(f"Extra√≠das {len(sections)} secciones del documento")
        
        ids_to_add, docs_to_add, metas_to_add = [], [], []
        existing_ids = set(self.collection.get(include=[])['ids'])
        
        for section in sections:
            # Crear chunks sem√°nticos para cada secci√≥n
            chunks = self._semantic_chunk(section, max_tokens=1000, overlap=75)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"enologia_{section['id'].replace('.', '_')}_{i}"
                
                if chunk_id not in existing_ids:
                    # Preparar contenido del chunk con contexto
                    chunk_content = f"Secci√≥n {section['id']}: {chunk['full_title']}\n\n{chunk['content']}"
                    
                    # Extraer keywords
                    keywords = self._extract_topic_keywords(chunk['content'])
                    
                    # Metadatos ricos
                    metadata = {
                        'type': 'knowledge',
                        'category': 'enologia',
                        'section_id': section['id'],
                        'section_title': section['main_title'],
                        'subsection_title': section['sub_title'],
                        'chunk_index': i,
                        'keywords': keywords,
                        'estimated_tokens': chunk['tokens']
                    }
                    
                    ids_to_add.append(chunk_id)
                    docs_to_add.append(chunk_content)
                    metas_to_add.append(metadata)
        
        if ids_to_add:
            self.collection.add(ids=ids_to_add, documents=docs_to_add, metadatas=metas_to_add)
            logger.info(f"‚úÖ A√±adidos {len(ids_to_add)} chunks de conocimiento enol√≥gico")
        else:
            logger.info("‚úÖ El conocimiento enol√≥gico ya estaba cargado")

    def _load_initial_data(self):
        """Carga la base de conocimiento inicial desde archivos JSON y texto."""
        # Cargar vinos desde JSON
        vinos_path = Path(__file__).parent / "knowledge_base" / "vinos.json"
        if vinos_path.exists():
            logger.info(f"Cargando base de vinos desde {vinos_path}...")
            with open(vinos_path, 'r', encoding='utf-8') as f:
                vinos = json.load(f)
            
            ids_to_add, docs_to_add, metas_to_add = [], [], []
            existing_ids = set(self.collection.get(include=[])['ids'])

            for i, vino in enumerate(vinos):
                doc_id = f"vino_{i}_{vino.get('name', 'unknown').replace(' ', '_').lower()}"
                if doc_id not in existing_ids:
                    # Estrategia de chunking para vinos (una entidad por chunk)
                    content_parts = [
                        f"Vino: {vino.get('name')}",
                        f"Tipo: {vino.get('type')}",
                        f"Bodega: {vino.get('winery')}",
                        f"Regi√≥n: {vino.get('region')}",
                        f"Uva: {vino.get('grape')}",
                        f"Graduaci√≥n: {vino.get('alcohol')}%",
                        f"Temperatura de servicio: {vino.get('temperature')}",
                        f"Crianza: {vino.get('crianza')}" if vino.get('crianza') else None,
                        f"Precio: {vino.get('price')}‚Ç¨",
                        f"Puntuaci√≥n: {vino.get('rating')}/100",
                        f"Maridaje: {vino.get('pairing')}",
                        f"Descripci√≥n: {vino.get('description')}"
                    ]
                    
                    # Filtrar partes vac√≠as y unir
                    content = ". ".join([part for part in content_parts if part and str(part) != "None"])
                    content += "."
                    
                    # A√±adir marcador de tipo para distinguir en metadatos
                    vino['type_content'] = 'wine'
                    
                    ids_to_add.append(doc_id)
                    docs_to_add.append(content)
                    metas_to_add.append(vino)

            if ids_to_add:
                self.collection.add(ids=ids_to_add, documents=docs_to_add, metadatas=metas_to_add)
                logger.info(f"‚úÖ A√±adidos {len(ids_to_add)} nuevos vinos")
        
        # Cargar conocimiento enol√≥gico desde texto con estrategia simple
        enology_path = Path(__file__).parent / "knowledge_base" / "maestria_enologica.txt"
        if enology_path.exists():
            self._process_enology_simple(enology_path)
        else:
            logger.warning(f"No se encontr√≥ el archivo de maestr√≠a enol√≥gica en {enology_path}")
        
        total_docs = self.collection.count()
        logger.info(f"‚úÖ Base de conocimiento cargada. Total documentos: {total_docs}")

    def search(self, query: str, max_results: int = 3) -> List[Dict]:
        """Realiza una b√∫squeda sem√°ntica en la colecci√≥n con filtros inteligentes."""
        
        # Detectar si la consulta es sobre conocimiento general o vinos espec√≠ficos
        knowledge_keywords = [
            'qu√© es', 'c√≥mo', 'por qu√©', 'cu√°ndo', 'd√≥nde', 'principios', 't√©cnica', 'm√©todo',
            'sumiller', 'maridaje', 'cata', 'servicio', 'proceso', 'historia', 'definici√≥n'
        ]
        
        wine_type_filters = {
            'tinto': 'Tinto', 'blanco': 'Blanco', 'rosado': 'Rosado',
            'champagne': 'Champagne', 'cava': 'Cava', 'fino': 'Fino',
            'manzanilla': 'Manzanilla', 'amontillado': 'Amontillado', 'oloroso': 'Oloroso'
        }
        
        query_lower = query.lower()
        
        # Determinar el tipo de b√∫squeda
        is_knowledge_query = any(keyword in query_lower for keyword in knowledge_keywords)
        wine_type_filter = None
        
        # Buscar tipo espec√≠fico de vino en la consulta
        for keyword, wine_type in wine_type_filters.items():
            if keyword in query_lower:
                wine_type_filter = wine_type
                break
        
        query_embedding = self.model.encode(query).tolist()
        
        # Aplicar filtros seg√∫n el tipo de consulta
        if is_knowledge_query:
            logger.info(f"üß† B√∫squeda de conocimiento: {query}")
            # Priorizar chunks de conocimiento
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=max_results * 2,
                include=["metadatas", "distances"],
                where={"type": "knowledge"}
            )
        elif wine_type_filter:
            logger.info(f"üç∑ B√∫squeda filtrada por tipo de vino: {wine_type_filter}")
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=max_results * 2,
                include=["metadatas", "distances"],
                where={"type": wine_type_filter}
            )
        else:
            # B√∫squeda general sin filtros
            logger.info(f"üîç B√∫squeda general: {query}")
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=max_results,
                include=["metadatas", "distances"]
            )
        
        formatted_results = []
        if results and results.get('ids')[0]:
            for metadata, distance in zip(results['metadatas'][0], results['distances'][0]):
                search_result = metadata.copy()
                search_result['relevance_score'] = 1 - distance
                formatted_results.append(search_result)
        
        # Limitar a los resultados solicitados
        return formatted_results[:max_results]

# Inicializaci√≥n del servicio
app = FastAPI(title="Agentic RAG Service", version="1.0.0")
rag_service = RAGService()

@app.get("/health")
async def health():
    """Health check endpoint."""
    return {"status": "healthy", "service": "agentic-rag"}

@app.get("/debug/chunks")
async def debug_chunks(limit: int = 5):
    """Debug endpoint para ver chunks."""
    try:
        results = rag_service.collection.get(limit=limit, include=['documents', 'metadatas'])
        return {
            "total_chunks": len(results['ids']),
            "sample_chunks": [
                {
                    "id": results['ids'][i],
                    "content": results['documents'][i],
                    "metadata": results['metadatas'][i] if results['metadatas'] else None
                }
                for i in range(min(limit, len(results['ids'])))
            ]
        }
    except Exception as e:
        logger.error(f"Error en debug_chunks: {e}")
        return {"error": str(e)}

@app.post("/search")
def search_endpoint(request: QueryRequest = Body(...)):
    """Endpoint para realizar b√∫squedas sem√°nticas."""
    try:
        results = rag_service.search(request.query, request.max_results)
        return {"wines": results}
    except Exception as e:
        logger.error(f"Error en el endpoint de b√∫squeda RAG: {e}")
        raise HTTPException(status_code=500, detail=str(e))
